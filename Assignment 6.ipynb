# Step 1: Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# Step 2: Load dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Step 3: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Define models
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier()
}

# Step 6: Evaluate models
def evaluate_model(name, model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"=== {name} ===")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1-Score:", f1_score(y_test, y_pred))
    print()

# Step 7: Run evaluation
for name, model in models.items():
    evaluate_model(name, model, X_train, X_test, y_train, y_test)

# Step 8: Hyperparameter Tuning

# Random Forest - Grid Search
param_grid_rf = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}
grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='f1', n_jobs=-1)
grid_rf.fit(X_train, y_train)
best_rf = grid_rf.best_estimator_
print("Best RF Params:", grid_rf.best_params_)
evaluate_model("Random Forest (Tuned)", best_rf, X_train, X_test, y_train, y_test)

# SVM - Randomized Search
param_dist_svm = {
    'C': np.logspace(-3, 2, 6),
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}
random_svm = RandomizedSearchCV(SVC(), param_distributions=param_dist_svm, cv=5, n_iter=10, scoring='f1', random_state=42, n_jobs=-1)
random_svm.fit(X_train, y_train)
best_svm = random_svm.best_estimator_
print("Best SVM Params:", random_svm.best_params_)
evaluate_model("SVM (Tuned)", best_svm, X_train, X_test, y_train, y_test)

# Step 9: Final Model Selection
print("ðŸŽ¯ Final Selection Based on F1-Score:")
models_final = {
    "Logistic Regression": LogisticRegression(),
    "SVM (Tuned)": best_svm,
    "Random Forest (Tuned)": best_rf,
    "KNN": KNeighborsClassifier()
}
scores = {}

for name, model in models_final.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    scores[name] = f1_score(y_test, y_pred)

best_model_name = max(scores, key=scores.get)
print(f"âœ… Best Model: {best_model_name} with F1-Score = {scores[best_model_name]:.4f}")